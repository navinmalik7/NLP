{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['NLP is an exciting field of AI.', 'It helps machines understand human language.']\n",
      "Word Tokenization: ['NLP', 'is', 'an', 'exciting', 'field', 'of', 'AI', '.', 'It', 'helps', 'machines', 'understand', 'human', 'language', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\navin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"NLP is an exciting field of AI. It helps machines understand human language.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\", sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokenization:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\navin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit', '-', 'Hole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into']\n"
     ]
    }
   ],
   "source": [
    "import nltk  \n",
    "nltk.download('gutenberg')  \n",
    "from nltk.corpus import gutenberg  \n",
    "\n",
    "# List available books  \n",
    "print(gutenberg.fileids())  \n",
    "\n",
    "# Load the text of \"Alice in Wonderland\"  \n",
    "alice = gutenberg.words('carroll-alice.txt')  \n",
    "print(alice[:50])  # Print the first 50 words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP PROPN\n",
      "is AUX\n",
      "amazing ADJ\n",
      "! PUNCT\n",
      "Let VERB\n",
      "'s PRON\n",
      "explore VERB\n",
      "it PRON\n",
      "together ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy  \n",
    "\n",
    "# Download the English model\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load the English model  \n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "# Process a sample text  \n",
    "doc = nlp(\"NLP is amazing! Let's explore it together.\")  \n",
    "\n",
    "# Print tokens and their parts of speech  \n",
    "for token in doc:  \n",
    "    print(token.text, token.pos_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navin\\anaconda3\\envs\\pyspark_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 9.05k/9.05k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 1.03M/1.03M [00:01<00:00, 885kB/s]\n",
      "Downloading data: 100%|██████████| 127k/127k [00:00<00:00, 177kB/s]\n",
      "Downloading data: 100%|██████████| 129k/129k [00:00<00:00, 205kB/s]\n",
      "Generating train split: 100%|██████████| 16000/16000 [00:00<00:00, 859565.59 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 460330.79 examples/s]\n",
      "Generating test split: 100%|██████████| 2000/2000 [00:00<00:00, 133892.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i didnt feel humiliated', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  \n",
    "\n",
    "# Load the emotion dataset  \n",
    "dataset = load_dataset(\"emotion\")  \n",
    "\n",
    "# Explore the dataset  \n",
    "print(dataset['train'][0])  # Print the first example  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
